{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Image_captioning_1.2_Inference.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahid-kh/Image_Captioning_With-Attention/blob/master/Inference/Image_captioning_1_2_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPDiuCbL593J",
        "colab_type": "text"
      },
      "source": [
        "# Attention based Image Captioning inferences with pre-trained models with Google text to speech."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3SHhZKU6dYL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "1242f820-9b04-4220-d63a-dccf7dbb6ac5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGLdgpK26X_o",
        "colab_type": "text"
      },
      "source": [
        "# Install Google text to Speech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLaKngLrCu1U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "1c9a2d6b-67c0-41ac-ad29-1ce317d6701d"
      },
      "source": [
        "!pip install gTTS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gTTS\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/0c/4ca77eca3b739a4a08360930643f58d714e302fee0d2f8c654e67d9af8e7/gTTS-2.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from gTTS) (7.1.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from gTTS) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gTTS) (1.12.0)\n",
            "Collecting gtts-token>=1.1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/25/ca6e9cd3275bfc3097fe6b06cc31db6d3dfaf32e032e0f73fead9c9a03ce/gTTS-token-1.1.3.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gTTS) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gTTS) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gTTS) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gTTS) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gTTS) (2.9)\n",
            "Building wheels for collected packages: gtts-token\n",
            "  Building wheel for gtts-token (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gtts-token: filename=gTTS_token-1.1.3-cp36-none-any.whl size=4097 sha256=3c1ad34aca0cf49b5de2f22815a01c7e0c8503909d739cf84a9c3a0c5f02d606\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/11/61/33f7e51bf545e910552b2255eead2a7cd8ef54064b46dceb34\n",
            "Successfully built gtts-token\n",
            "Installing collected packages: gtts-token, gTTS\n",
            "Successfully installed gTTS-2.1.1 gtts-token-1.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2PLDLQF8POE",
        "colab_type": "text"
      },
      "source": [
        "# Necessary imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U8l4RJ0XRPEm",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# You'll generate plots of attention in order to see which parts of an image\n",
        "# our model focuses on during captioning\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scikit-learn includes many helpful utilities\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from gtts import gTTS #Import Google Text to Speech\n",
        "from IPython.display import Audio #Import Audio method from IPython's Display Class\n",
        "from PIL import ImageFont\n",
        "from PIL import ImageDraw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-sKSGF86gVQ",
        "colab_type": "text"
      },
      "source": [
        "#Image Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zXR0217aRPFR",
        "colab": {}
      },
      "source": [
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img, image_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJoc9c1s6mFJ",
        "colab_type": "text"
      },
      "source": [
        "#Using Pre-trained Inception V3 as image feature extractor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RD3vW4SsRPFW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "3b4a4f08-5cd9-4667-817e-981fb9b6a43b"
      },
      "source": [
        "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                                weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q3TnZ1ToRPGV",
        "colab": {}
      },
      "source": [
        "# Feel free to change these parameters according to your system's configuration\n",
        "top_k = 5000\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_size = top_k + 1\n",
        "#num_steps = len(img_name_train) // BATCH_SIZE\n",
        "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
        "# These two variables represent that vector shape\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64\n",
        "max_length=49"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SikTkynI6x6_",
        "colab_type": "text"
      },
      "source": [
        "# Encoder,Decoder,and Attention Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ja2LFTMSdeV3",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, features, hidden):\n",
        "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
        "\n",
        "    # hidden shape == (batch_size, hidden_size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "    # score shape == (batch_size, 64, hidden_size)\n",
        "    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
        "\n",
        "    # attention_weights shape == (batch_size, 64, 1)\n",
        "    # you get 1 at the last axis because you are applying score to self.V\n",
        "    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * features\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AZ7R1RxHRPGf",
        "colab": {}
      },
      "source": [
        "class CNN_Encoder(tf.keras.Model):\n",
        "    # Since you have already extracted the features and dumped it using pickle\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        # shape after fc == (batch_size, 64, embedding_dim)\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V9UbGQmERPGi",
        "colab": {}
      },
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units, vocab_size):\n",
        "    super(RNN_Decoder, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "  def call(self, x, features, hidden):\n",
        "    # defining attention as a separate model\n",
        "    context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # shape == (batch_size, max_length, hidden_size)\n",
        "    x = self.fc1(output)\n",
        "\n",
        "    # x shape == (batch_size * max_length, hidden_size)\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size * max_length, vocab)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "  def reset_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtBgBCJpGaG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "#load saved wieghts\n",
        "load_path='/content/drive/My Drive/image_captioning'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYnVT3Ec79b-",
        "colab_type": "text"
      },
      "source": [
        "#Give Paths to your h5 files and pickled tokenizer downloaded from Repo "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsO7kSoaey1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "encoder.load_weights('/content/drive/My Drive/image_captioning/encoder.h5',by_name=True)\n",
        "decoder.load_weights('/content/drive/My Drive/image_captioning/decoder.h5',by_name=True)\n",
        "with open('/content/drive/My Drive/image_captioning/tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "\n",
        "# Give the image path\n",
        "image_path='/content/drive/My Drive/image_captioning/IMG_20180901_185257.jpg'\n",
        "result, attention_plot = evaluate(image_path)\n",
        "\n",
        "#You wil see an error here after running above,dont worry jst proceed further\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLnUjXnc7hTU",
        "colab_type": "text"
      },
      "source": [
        "# Reload weights and try again if you encountered in previous step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlxZzsCWHPhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "encoder.load_weights('/content/drive/My Drive/image_captioning/encoder.h5')\n",
        "decoder.load_weights('/content/drive/My Drive/image_captioning/decoder.h5')\n",
        "with open('/content/drive/My Drive/image_captioning/tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RCWpDtyNRPGs",
        "colab": {}
      },
      "source": [
        "def evaluate(image):\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
        "\n",
        "    features = encoder(img_tensor_val)\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "\n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "        result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "        if tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, attention_plot\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    attention_plot = attention_plot[:len(result), :]\n",
        "    return result, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Psd1quzaAWg",
        "colab": {}
      },
      "source": [
        "#Give your own imag path\n",
        "image_path='/content/drive/My Drive/image_captioning/IMG_20180901_185257.jpg'\n",
        "result, attention_plot = evaluate(image_path)\n",
        "\n",
        "output=' '.join(result)\n",
        "\n",
        "output=re.sub('<end>',' ', output )\n",
        "\n",
        "#print ('Prediction Caption:', ' '.join(result))\n",
        "print ('Prediction Caption:',output)\n",
        "#plot_attention(image_path, result, attention_plot)\n",
        "# opening the image\n",
        "size=128,128\n",
        "outpath='/content/img.jpg'\n",
        "im=Image.open(image_path)\n",
        "im=im.resize((300,300),Image.ANTIALIAS)\n",
        "im.save(outpath)\n",
        "Image.open(outpath)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veYRF8FV5TCu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "e89e6608-8cc4-4a4f-bf8a-30e8d76ee6b5"
      },
      "source": [
        "tts = gTTS(output) #Provide the string to convert to speech\n",
        "tts.save('1.wav') #save the string converted to speech as a .wav file\n",
        "sound_file = '1.wav'\n",
        "Audio(sound_file, autoplay=True) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "                <audio controls=\"controls\" autoplay=\"autoplay\">\n",
              "                    <source src=\"data:audio/x-wav;base64,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\" type=\"audio/x-wav\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ],
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    }
  ]
}